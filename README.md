# ğŸš€ process-1B-records-kafka-spark

### Ultra High-Throughput Streaming Pipeline Using Kafka + Schema Registry + Spark + Delta Lake

This project demonstrates how to build a production-grade streaming data pipeline capable of processing **1 billion records per hour (~277,777 records per second)** using cutting-edge open-source tools and cloud infrastructure.

---

## âš™ï¸ Stack Summary

| Layer              | Tool/Service                          |
|--------------------|----------------------------------------|
| Ingestion Engine   | Apache Kafka (KRaft mode, no Zookeeper) |
| Serialization      | Apache Avro + Confluent Schema Registry |
| Stream Processor   | Apache Spark Structured Streaming (Databricks) |
| Storage Layer      | Delta Lake on AWS S3 / cloud object store |
| Orchestration      | Apache Airflow (planned)                |
| Monitoring         | Prometheus + Grafana (planned)          |
| Dev Environment    | Docker Compose + Python + GitHub        |

---

## ğŸ’¡ Project Goals

- Simulate **high-velocity data streams** in Avro format.
- Validate messages against **Avro schemas** with Schema Registry.
- **Ingest into Kafka** at hundreds of thousands of messages per second.
- Consume using **Spark Structured Streaming** on Databricks.
- Write results to **Delta Lake** with ACID guarantees.
- Enable **real-time analytics** on massive datasets.
- Showcase **production-like architecture**, **performance tuning**, and **scalability**.

---

## ğŸ§± Architecture Overview

\`\`\`
+--------------------+        +--------------------+         +--------------------+
| High-throughput    | -----> |   Kafka Topic      | ----->  |  Spark on Databricks|
| Python Producer    |  Avro  | (100+ partitions)  |         |  (Structured Stream)|
+--------------------+        +--------------------+         +---------+----------+
                                                               |
                                                               v
                                                  +------------------------+
                                                  |   Delta Lake on S3     |
                                                  +------------------------+
                                                               |
                                                               v
                                                  +------------------------+
                                                  |  Power BI / Tableau    |
                                                  +------------------------+
\`\`\`

---

## ğŸ“¦ Project Structure

\`\`\`
process-1B-records-kafka-spark/
â”‚
â”œâ”€â”€ docker-compose.yml         # KRaft Kafka + Schema Registry
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ producer/                  # Avro-based producer (Python)
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ user_event.avsc
â”‚
â”œâ”€â”€ databricks_jobs/           # Spark Structured Streaming scripts / notebooks
â”‚
â”œâ”€â”€ dashboards/                # BI dashboard files (Power BI / Tableau)
â”‚
â””â”€â”€ README.md
\`\`\`

---

## ğŸš€ Getting Started

### 1. ğŸ³ Run Kafka and Schema Registry

\`\`\`bash
docker-compose up -d
\`\`\`

Kafka will be available at \`localhost:9092\`, and Schema Registry at \`localhost:8081\`.

---

### 2. ğŸ§ª Produce Data to Kafka

\`\`\`bash
cd producer
python producer.py
\`\`\`

This script generates hundreds of thousands of Avro-encoded events per second and sends them to a Kafka topic (\`avro-topic\`), with schema auto-registered in Schema Registry.

---

### 3. ğŸ”¥ Process with Spark (on Databricks)

- Import the job from \`databricks_jobs/\` into your Databricks workspace.
- Set up Kafka connection (public broker endpoint or port-forwarding).
- Start streaming the Kafka topic and write output to Delta Lake.

---

### 4. ğŸ“Š Analyze

- Connect your dashboard tool (e.g., Power BI, Tableau) to Delta tables.
- Build real-time KPIs like:
  - Processed events per second
  - Volume per region/category/device
  - Streaming lag / errors

---

## ğŸ” Status

| Component                | Status     |
|--------------------------|------------|
| Kafka (KRaft)            | âœ… Running |
| Schema Registry          | âœ… Running |
| Avro Producer            | ğŸ”„ In Progress |
| Spark Consumer           | ğŸ”œ Next |
| Delta Lake Integration   | ğŸ”œ Next |
| BI Dashboard             | ğŸ”œ Planned |
| Monitoring Stack         | ğŸ”œ Planned |
| FastAPI Admin Panel      | âŒ Future |

---

## ğŸ“œ License

MIT Â© [Paritosh Ghimire](https://github.com/psgpyc)
